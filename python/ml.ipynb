{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Done by: [Nicholas Tan Qin Sheng] and [Muhammad Hafiz Bin Abdul Halim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, ndcg_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HexMa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HexMa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HexMa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data (if you haven't already)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "def load_and_prepare_data(filepath, sample_size=None):\n",
    "    \"\"\"Loads JSONL data, preprocesses review text, and adds a sample size for testing.\"\"\"\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "\n",
    "    if sample_size:\n",
    "        df = df.sample(sample_size, random_state=42) # For reproducibility\n",
    "    df = df[['asin', 'text', 'rating']] # Select relevant columns\n",
    "    df.rename(columns={'text': 'review_text'}, inplace=True)  # Rename columns\n",
    "    df.dropna(subset=['review_text'], inplace=True) # Drop rows with NaN review text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess Review Text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and lemmatizes the review text.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE) # Remove punctuation\n",
    "    text = text.lower() # Lowercase\n",
    "    stop_words = set(stopwords.words('english')) # Stop words\n",
    "    text = [w for w in text.split() if not w in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer() # Lemmatize\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Engineering\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Creates TF-IDF vectors from review text and adds a simple sentiment score.\"\"\"\n",
    "    df['cleaned_review_text'] = df['review_text'].apply(preprocess_text) # Clean text\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  # Limit vocabulary size\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cleaned_review_text'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df.index)  # TF-IDF to DataFrame\n",
    "\n",
    "    df['sentiment'] = df['rating'].apply(lambda x: 1 if x > 3 else (-1 if x < 3 else 0))  # Simplified sentiment\n",
    "    return df, tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Content-Based Recommendation System\n",
    "def create_recommendation_system(df, tfidf_df):\n",
    "    \"\"\"Calculates cosine similarity and recommends products based on content.\"\"\"\n",
    "    cosine_sim = cosine_similarity(tfidf_df, tfidf_df)\n",
    "    return cosine_sim\n",
    "\n",
    "def recommend_products(asin, df, cosine_sim, top_n=5):\n",
    "    \"\"\"Recommends similar products based on a given ASIN.\"\"\"\n",
    "    try:\n",
    "        idx = df[df['asin'] == asin].index[0] # Get index of product\n",
    "    except IndexError:\n",
    "        return f\"Product '{asin}' not found.\"\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # Similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Sort\n",
    "    sim_scores = sim_scores[1:top_n+1] # Get top N similar\n",
    "    product_indices = [i[0] for i in sim_scores] # Product indices\n",
    "    return df['asin'].iloc[product_indices].tolist() # Return ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate the Recommendation System (Simplified)\n",
    "def evaluate_recommendation_system(df, cosine_sim, test_size=0.2):\n",
    "  \"\"\"Splits data, makes recommendations, and evaluates using basic metrics.\"\"\"\n",
    "  train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "  def get_relevant_asins(asin, train_df, cosine_sim, top_n=5):\n",
    "    \"\"\"Gets a list of relevant ASINs for a given ASIN.\"\"\"\n",
    "    try:\n",
    "        idx = train_df[train_df['asin'] == asin].index[0]\n",
    "    except IndexError:\n",
    "        return []\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    return train_df['asin'].iloc[product_indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate the Recommendation System (Simplified)\n",
    "def evaluate_recommendation_system(df, cosine_sim, test_size=0.2):\n",
    "  \"\"\"Splits data, makes recommendations, and evaluates using basic metrics.\"\"\"\n",
    "  train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "  def get_relevant_asins(asin, df, train_df, cosine_sim, top_n=5):\n",
    "    \"\"\"Gets a list of relevant ASINs for a given ASIN.\"\"\"\n",
    "    try:\n",
    "        idx = df[df['asin'] == asin].index[0]\n",
    "    except IndexError:\n",
    "        return []\n",
    "    \n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter similar products to only those in the training set\n",
    "    recommended_asins = []\n",
    "    for i, _ in sim_scores[1:]:\n",
    "        asin_i = df['asin'].iloc[i]\n",
    "        if asin_i in train_df['asin'].values:\n",
    "            recommended_asins.append(asin_i)\n",
    "        if len(recommended_asins) >= top_n:\n",
    "            break\n",
    "\n",
    "    return recommended_asins\n",
    "\n",
    "  # Make recommendations for test set\n",
    "  recommendations[asin] = get_relevant_asins(asin, df, train_df, cosine_sim)\n",
    "\n",
    "  # Evaluate using simple metrics\n",
    "  precision_list = []\n",
    "  recall_list = []\n",
    "\n",
    "  for asin, recommended_asins in recommendations.items():\n",
    "    # Try to find the rating in train_df\n",
    "    try:\n",
    "        test_index = test_df[test_df['asin'] == asin].index[0]\n",
    "        rating_value = test_df['rating'].iloc[test_index]\n",
    "    except IndexError:\n",
    "        rating_value = 0\n",
    "    # Look if similar products in recommendations can also be found on rating\n",
    "    asin_with_rating = train_df[train_df['rating'] == rating_value]['asin'].tolist()\n",
    "\n",
    "    relevant_count = len(set(recommended_asins) & set(asin_with_rating)) # Find with recommendation and rating\n",
    "    if len(recommended_asins) > 0:\n",
    "        precision_list.append(relevant_count / len(recommended_asins))\n",
    "    else:\n",
    "        precision_list.append(0.0)  # Avoid division by zero\n",
    "\n",
    "    if len(asin_with_rating) > 0:\n",
    "        recall_list.append(relevant_count / len(asin_with_rating))\n",
    "    else:\n",
    "        recall_list.append(0.0)  # Avoid division by zero\n",
    "\n",
    "  # Handle empty lists\n",
    "  precision = sum(precision_list) / len(precision_list) if precision_list else 0.0\n",
    "  recall = sum(recall_list) / len(recall_list) if recall_list else 0.0\n",
    "\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HexMa\\Desktop\\inf2006-big-data\\python\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load, prepare, and engineer features\u001b[39;00m\n\u001b[32m      5\u001b[39m df = load_and_prepare_data(filepath, sample_size)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df, tfidf_df = \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create and evaluate the recommendation system\u001b[39;00m\n\u001b[32m      9\u001b[39m cosine_sim = create_recommendation_system(df, tfidf_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mfeature_engineering\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeature_engineering\u001b[39m(df):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates TF-IDF vectors from review text and adds a simple sentiment score.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(preprocess_text) \u001b[38;5;66;03m# Clean text\u001b[39;00m\n\u001b[32m      6\u001b[39m     vectorizer = TfidfVectorizer(max_features=\u001b[32m5000\u001b[39m)  \u001b[38;5;66;03m# Limit vocabulary size\u001b[39;00m\n\u001b[32m      7\u001b[39m     tfidf_matrix = vectorizer.fit_transform(df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_text\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HexMa\\Desktop\\inf2006-big-data\\python\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HexMa\\Desktop\\inf2006-big-data\\python\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "filepath = 'data/Video_Games_with_sentiment.jsonl'  # Replace with your actual file path\n",
    "sample_size = 5000  # Adjust for testing (remove for full dataset)\n",
    "\n",
    "# Load, prepare, and engineer features\n",
    "df = load_and_prepare_data(filepath, sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3695993 is out of bounds for axis 0 with size 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create and evaluate the recommendation system\u001b[39;00m\n\u001b[32m      4\u001b[39m cosine_sim = create_recommendation_system(df, tfidf_df)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mevaluate_recommendation_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Make recommendations for a specific product\u001b[39;00m\n\u001b[32m      8\u001b[39m asin_to_recommend = \u001b[33m'\u001b[39m\u001b[33mB00005B158\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Example ASIN (replace with a real one)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mevaluate_recommendation_system\u001b[39m\u001b[34m(df, cosine_sim, test_size)\u001b[39m\n\u001b[32m     19\u001b[39m recommendations = {}\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m asin \u001b[38;5;129;01min\u001b[39;00m test_df[\u001b[33m'\u001b[39m\u001b[33masin\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m   recommendations[asin] = \u001b[43mget_relevant_asins\u001b[49m\u001b[43m(\u001b[49m\u001b[43masin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Evaluate using simple metrics\u001b[39;00m\n\u001b[32m     24\u001b[39m precision_list = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_recommendation_system.<locals>.get_relevant_asins\u001b[39m\u001b[34m(asin, train_df, cosine_sim, top_n)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sim_scores = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mcosine_sim\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[32m     13\u001b[39m sim_scores = \u001b[38;5;28msorted\u001b[39m(sim_scores, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m sim_scores = sim_scores[\u001b[32m1\u001b[39m:top_n+\u001b[32m1\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: index 3695993 is out of bounds for axis 0 with size 5000"
     ]
    }
   ],
   "source": [
    "df, tfidf_df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_recommendation_system.<locals>.get_relevant_asins() missing 1 required positional argument: 'cosine_sim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create and evaluate the recommendation system\u001b[39;00m\n\u001b[32m      2\u001b[39m cosine_sim = create_recommendation_system(df, tfidf_df)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mevaluate_recommendation_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Make recommendations for a specific product\u001b[39;00m\n\u001b[32m      6\u001b[39m asin_to_recommend = \u001b[33m'\u001b[39m\u001b[33mB00005B158\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Example ASIN (replace with a real one)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mevaluate_recommendation_system\u001b[39m\u001b[34m(df, cosine_sim, test_size)\u001b[39m\n\u001b[32m     28\u001b[39m recommendations = {}\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m asin \u001b[38;5;129;01min\u001b[39;00m test_df[\u001b[33m'\u001b[39m\u001b[33masin\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m   recommendations[asin] = \u001b[43mget_relevant_asins\u001b[49m\u001b[43m(\u001b[49m\u001b[43masin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Evaluate using simple metrics\u001b[39;00m\n\u001b[32m     33\u001b[39m precision_list = []\n",
      "\u001b[31mTypeError\u001b[39m: evaluate_recommendation_system.<locals>.get_relevant_asins() missing 1 required positional argument: 'cosine_sim'"
     ]
    }
   ],
   "source": [
    "# Create and evaluate the recommendation system\n",
    "cosine_sim = create_recommendation_system(df, tfidf_df)\n",
    "evaluate_recommendation_system(df, cosine_sim)\n",
    "\n",
    "# Make recommendations for a specific product\n",
    "asin_to_recommend = 'B00005B158'  # Example ASIN (replace with a real one)\n",
    "recommendations = recommend_products(asin_to_recommend, df, cosine_sim)\n",
    "\n",
    "if isinstance(recommendations, str):  # Error message\n",
    "    print(recommendations)\n",
    "else:\n",
    "    print(f\"Recommended products for {asin_to_recommend}: {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
